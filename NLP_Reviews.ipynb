{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11tj-pjeXLb5PluGcxO0eQqqMesUo87SF",
      "authorship_tag": "ABX9TyOGMZLtsVIXoPXWFmxH39j1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RihaChri/NLPReviews/blob/main/NLP_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "UmfeVnLtwScl",
        "outputId": "4bccd1ea-8824-4191-81bd-e45905613b82"
      },
      "source": [
        "#Imports\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "import pandas as pd     \n",
        "\n",
        "#Keras Import\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Show version number\n",
        "print(\"Tensorflow version: \"+tf.__version__)\n",
        "print(\"Keras version:      \"+tf.keras.__version__)\n",
        "ShowCaseExample = 2;\n",
        "\n",
        "#Import of Dataset\n",
        "dataset_X=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP-MovieReviews/IMDB Dataset.csv', engine='python', usecols = [0])\n",
        "dataset_Y=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP-MovieReviews/IMDB Dataset.csv', engine='python', usecols = [1])\n",
        "\n",
        "#Split into train and test set\n",
        "splitVector = np.random.rand(len(dataset_Y)) <= 0.7\n",
        "train_X = dataset_X[splitVector]\n",
        "test_X = dataset_X[~splitVector]\n",
        "train_Y = dataset_Y[splitVector]\n",
        "test_Y = dataset_Y[~splitVector]\n",
        "\n",
        "#convert to numpy array\n",
        "train_X = np.array(train_X)\n",
        "test_X = np.array(test_X)\n",
        "train_Y = np.array(train_Y)\n",
        "test_Y = np.array(test_Y)\n",
        "dataset_X = np.array(dataset_X)\n",
        "dataset_Y = np.array(dataset_Y)\n",
        "\n",
        "#convert the Y strings 'positive' & 'negative' to 0 and 1\n",
        "for x in train_Y:\n",
        "   if str(x[0])=='positive': x[0]=1.0\n",
        "   if str(x[0])=='negative': x[0]=0.0\n",
        "for x in test_Y:\n",
        "   if str(x[0])=='positive': x[0]=1.0\n",
        "   if str(x[0])=='negative': x[0]=0.0\n",
        "\n",
        "train_Y = tf.convert_to_tensor(train_Y, dtype=tf.int64) \n",
        "test_Y = tf.convert_to_tensor(test_Y, dtype=tf.int64) \n",
        "\n",
        "print(\"Dimension of training set: X\"+str(train_X.shape)+\", Y\"+str(train_Y.shape))\n",
        "print(\"Dimension of test set: X\"+str(test_X.shape)+\", Y\"+str(test_Y.shape))\n",
        "\n",
        "#Tokenizer\n",
        "num_words = 10000\n",
        "#num_words = len(tokenizer.word_index)\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(dataset_X[:,0])\n",
        "tokenizer.word_index\n",
        "\n",
        "x_train_tokens = tokenizer.texts_to_sequences(train_X[:,0])\n",
        "x_test_tokens = tokenizer.texts_to_sequences(test_X[:,0])\n",
        "\n",
        "#checkout an example\n",
        "print(\"Example review: \"+str(train_X[ShowCaseExample,0]))\n",
        "print(\"This example is: \"+str(train_Y[ShowCaseExample,0]))\n",
        "print(\"word index: \"+str(tokenizer.word_index))\n",
        "print(\"The indexed version of this is:\\n \"+str(x_train_tokens[ShowCaseExample]))\n",
        "\n",
        "#number of tokens\n",
        "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens] #returns a list with number of tokens for each X example\n",
        "num_tokens = np.array(num_tokens)\n",
        "print(\"number of tokens (words) in this example: \"+str(num_tokens[ShowCaseExample]))\n",
        "print(\"\\n\\nAverage number of tokens (words):\"+str(np.mean(num_tokens)))\n",
        "print(\"Max number of tokens (words):\"+str(np.max(num_tokens)))\n",
        "\n",
        "#Max allowed number of tokens\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "print(\"Allowed number of tokens (words): \"+str(max_tokens))\n",
        "print(\"This covers about \"+str(np.sum(num_tokens < max_tokens) / len(num_tokens))+\" of the dataset is covered\")\n",
        "\n",
        "\n",
        "#Padding\n",
        "pad = 'pre' # Means that we add indexes \"0\" or throw away indexes at the beginning (pre) or end (post) of an example\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "print(\"Train pad shape: \"+str(x_train_pad.shape))\n",
        "print(\"Test pad shape: \"+str(x_test_pad.shape))\n",
        "print(\"The padded version of this example is: \"+str(x_train_pad[ShowCaseExample,:]))\n",
        "\n",
        "#Tokenizer Inverse Map\n",
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "    \n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "print(\"inversing indices of the example yields: \"+str(tokens_to_string(x_train_tokens[ShowCaseExample])))\n",
        "\n",
        "#Create the Recurrent Neural Network\n",
        "model = Sequential()\n",
        "embedding_size = 8\n",
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens,\n",
        "                    name='layer_embedding'))\n",
        "model.add(GRU(units=16, return_sequences=True))\n",
        "model.add(GRU(units=8, return_sequences=True))\n",
        "model.add(GRU(units=4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "optimizer = Adam(lr=1e-3)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#---------------------------\n",
        "#Train the Recurrent Neural Network\n",
        "model.fit(x_train_pad, train_Y[:,0], validation_split=0.05, epochs=3, batch_size=64)\n",
        "#Performance on Test-Set\n",
        "result = model.evaluate(x_test_pad, test_Y[:,0])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version: 2.7.0\n",
            "Keras version:      2.7.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f5182f296f70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#Import of Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdataset_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./NLP-MovieReviews/IMDB Dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdataset_Y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./NLP-MovieReviews/IMDB Dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 )\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2387\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m         )\n\u001b[1;32m   2391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './NLP-MovieReviews/IMDB Dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbiZ48oalwk1",
        "outputId": "27675a8e-93f1-48b7-b363-9e4f1f266799"
      },
      "source": [
        "\n",
        "#Example of Mis-Classified Text\n",
        "y_pred = model.predict(x=x_test_pad[0:1000])#predict first 1000 examples\n",
        "y_pred = y_pred.T[0]\n",
        "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])#all above 0.5 are 1 else 0\n",
        "cls_true = np.array(test_Y[0:1000,0])                       #show all that are actually true\n",
        "incorrect = np.where(cls_pred != cls_true)                  #indices of the incorrect ones\n",
        "incorrect = incorrect[0]                                    #take the first incorrect example\n",
        "print(\"L채nge des falschen Beispiels: \"+ str(len(incorrect)))\n",
        "idx = incorrect[0]\n",
        "print(\"Index des falschen Beispiels: \"+str(idx))\n",
        "text = test_X[idx]\n",
        "print(\"Text des falschen Beispiels: \"+str(text))\n",
        "print(\"Vorhersage des falschen Beispiels:\"+str(y_pred[idx]))\n",
        "print(\"Tats채chlicher Wert des falschen Beispiels: \"+str(cls_true[idx]))\n",
        "\n",
        "#New Data\n",
        "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "text2 = \"Good movie!\"\n",
        "text3 = \"Maybe I like this movie.\"\n",
        "text4 = \"Meh ...\"\n",
        "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
        "text6 = \"Bad movie!\"\n",
        "text7 = \"Not a good movie!\"\n",
        "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "text9 = \"This movie really moved me to tears.\"\n",
        "text10 = \"What a waste of time\"\n",
        "texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10]\n",
        "\n",
        "tokens = tokenizer.texts_to_sequences(texts)\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
        "                           padding=pad, truncating=pad)\n",
        "print(tokens_pad.shape)\n",
        "\n",
        "print(model.predict(tokens_pad))\n",
        "\n",
        "#Embedding\n",
        "layer_embedding = model.get_layer('layer_embedding')\n",
        "weights_embedding = layer_embedding.get_weights()[0]\n",
        "weights_embedding.shape\n",
        "token_good = tokenizer.word_index['good']\n",
        "print(token_good)\n",
        "token_great = tokenizer.word_index['great']\n",
        "print(token_great)\n",
        "weights_embedding[token_good]\n",
        "weights_embedding[token_great]\n",
        "token_bad = tokenizer.word_index['bad']\n",
        "token_horrible = tokenizer.word_index['horrible']\n",
        "weights_embedding[token_bad]\n",
        "weights_embedding[token_horrible]\n",
        "\n",
        "\n",
        "#Sorted Words\n",
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = weights_embedding[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(weights_embedding, [embedding],\n",
        "                      metric=metric).T[0]\n",
        "    \n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "    \n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "    \n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [inverse_map[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for word, distance in zip(words, distances):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])\n",
        "\n",
        "print_sorted_words('great', metric='cosine')\n",
        "print_sorted_words('worst', metric='cosine')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L채nge des falschen Beispiels: 121\n",
            "Index des falschen Beispiels: 14\n",
            "Text des falschen Beispiels: [\"Bela Lugosi appeared in several of these low budget chillers for Monogram Studios in the 1940's and The Corpse Vanishes is one of the better ones.<br /><br />Bela plays a mad scientist who kidnaps young brides and kills them and then extracts fluid from their bodies so he can keep his ageing wife looking young. After a reporter and a doctor stay the night at his home and discover he is responsible for the brides' deaths, the following morning they report these murders to the police and the mad scientist is shot and drops dead shortly afterwards.<br /><br />You have got almost everything in this movie: the scientist's assistants consist of an old hag, a hunchback and dwarf (her sons), a thunderstorm and spooky passages in Bela's house. Bela and his wife find they sleep better in coffins rather than beds in the movie.<br /><br />The Corpse Vanishes is worth a look, especially for Bela Lugosi fans. Great fun.<br /><br />Rating: 3 stars out of 5.\"]\n",
            "Vorhersage des falschen Beispiels:0.30227724\n",
            "Tats채chlicher Wert des falschen Beispiels: 1\n",
            "(10, 544)\n",
            "[[0.9186462 ]\n",
            " [0.73841906]\n",
            " [0.48200858]\n",
            " [0.73717505]\n",
            " [0.32210213]\n",
            " [0.23018157]\n",
            " [0.6670943 ]\n",
            " [0.24995473]\n",
            " [0.82409376]\n",
            " [0.0657469 ]]\n",
            "49\n",
            "78\n",
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.010 - simpler\n",
            "0.010 - perfect\n",
            "0.011 - heartbreaking\n",
            "0.011 - conflicted\n",
            "0.011 - antwone\n",
            "0.013 - garner\n",
            "0.013 - terrific\n",
            "0.013 - parker\n",
            "0.013 - friendship\n",
            "...\n",
            "1.990 - unnecessary\n",
            "1.990 - button\n",
            "1.990 - sorry\n",
            "1.990 - fails\n",
            "1.990 - scattered\n",
            "1.991 - moviegoers\n",
            "1.991 - marathon\n",
            "1.991 - meandering\n",
            "1.992 - horrible\n",
            "1.995 - themed\n",
            "Distance from 'worst':\n",
            "0.000 - worst\n",
            "0.002 - fails\n",
            "0.003 - waste\n",
            "0.003 - button\n",
            "0.004 - 0\n",
            "0.004 - 1993\n",
            "0.004 - ludicrous\n",
            "0.004 - earnest\n",
            "0.004 - ripoff\n",
            "0.005 - horrible\n",
            "...\n",
            "1.993 - gem\n",
            "1.993 - friendship\n",
            "1.993 - touching\n",
            "1.994 - fantastic\n",
            "1.994 - exceptional\n",
            "1.994 - locke\n",
            "1.995 - heartbreaking\n",
            "1.995 - sabrina\n",
            "1.995 - plans\n",
            "1.998 - today's\n"
          ]
        }
      ]
    }
  ]
}